{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import augraphy as ag\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import Adam\n",
    "from torch.optim import AdamW\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from PIL import ImageOps\n",
    "import re \n",
    "from PIL import ImageEnhance, ImageFilter\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from collections import Counter\n",
    "import pytesseract\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#메모리 초기화\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시드를 고정합니다.\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 클래스를 정의합니다.\n",
    "#oversampling -> 클래스 빈 130개 채워서 1700개 : max_oversample 로 배율 조정\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv, path, transform=None, oversample=False, class_transforms=None, max_oversample=4, use_ocr=False):\n",
    "        self.df = pd.read_csv(csv)\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "        self.class_transforms = class_transforms  # 추가된 부분\n",
    "        self.oversample = oversample\n",
    "        self.max_oversample = max_oversample\n",
    "        self.use_ocr = use_ocr\n",
    "\n",
    "        # OCR 대상 클래스 텍스트 추출 및 벡터화\n",
    "        if self.use_ocr:\n",
    "            ocr_texts = [\n",
    "                extract_text_from_image(os.path.join(self.path, row['ID'])) \n",
    "                if row['target'] in [3, 4, 7, 14] else \"\" \n",
    "                for _, row in self.df.iterrows()\n",
    "            ]\n",
    "            self.text_vectors = text_to_vector(ocr_texts)\n",
    "        else:\n",
    "            self.text_vectors = [np.zeros(50) for _ in range(len(self.df))]  # OCR 비활성화 시 기본 0 벡터 사용\n",
    "            \n",
    "        if self.oversample:\n",
    "            self.df, self.text_vectors = self.apply_oversampling(self.df, self.text_vectors)\n",
    "\n",
    "    def apply_oversampling(self, df, text_vectors):\n",
    "        class_counts = Counter(df['target'])\n",
    "        max_count = int(max(class_counts.values()) * self.max_oversample)  # 설정한 배수만큼 샘플 수 제한\n",
    "        oversampled_df = df.copy()\n",
    "        oversampled_text_vectors = text_vectors[:]\n",
    "\n",
    "        for cls, count in class_counts.items():\n",
    "            if count < max_count:\n",
    "                # 부족한 샘플 수만큼 추가 복제\n",
    "                samples_to_add = df[df['target'] == cls]\n",
    "                text_vectors_to_add = [text_vectors[i] for i in samples_to_add.index]\n",
    "                \n",
    "                for _ in range(max_count // count - 1):  # 배수만큼 추가\n",
    "                    oversampled_df = pd.concat([oversampled_df, samples_to_add])\n",
    "                    oversampled_text_vectors.extend(text_vectors_to_add)\n",
    "                \n",
    "                # 나머지 추가 복제\n",
    "                remainder = max_count % count\n",
    "                if remainder > 0:\n",
    "                    oversampled_df = pd.concat([oversampled_df, samples_to_add.sample(remainder, replace=True)])\n",
    "                    oversampled_text_vectors.extend(text_vectors_to_add[:remainder])\n",
    "\n",
    "        return oversampled_df.sample(frac=1).reset_index(drop=True), oversampled_text_vectors\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        image_name = self.df.iloc[idx]['ID']\n",
    "        target = self.df.iloc[idx]['target']\n",
    "        img_path = os.path.join(self.path, image_name)\n",
    "        img = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        \n",
    "        # 클래스별로 다른 transform 적용\n",
    "        if self.class_transforms and target in self.class_transforms:\n",
    "            img = self.class_transforms[target](image=img)['image']  # 변경된 부분\n",
    "        elif self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        \n",
    "        # OCR 텍스트 벡터 가져오기\n",
    "        text_vector = self.text_vectors[idx] if self.use_ocr and target in [3, 4, 7, 14] else np.zeros(50)\n",
    "        \n",
    "        return img, torch.tensor(text_vector, dtype=torch.float32), target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def preprocess_image_for_ocr(img):\n",
    "    # 이미지 확대\n",
    "    img = img.resize((img.width * 2, img.height * 2), Image.LANCZOS)\n",
    "    \n",
    "    # 밝기 및 대비 조절\n",
    "    enhancer = ImageEnhance.Contrast(img)\n",
    "    img = enhancer.enhance(2)  # 대비 증가\n",
    "    enhancer = ImageEnhance.Brightness(img)\n",
    "    img = enhancer.enhance(1.5)  # 밝기 증가\n",
    "    \n",
    "    # 이미지 날카로움 증가\n",
    "    img = img.filter(ImageFilter.SHARPEN)\n",
    "\n",
    "    # 그레이스케일 및 이진화\n",
    "    img = img.convert('L')  # 그레이스케일로 변환\n",
    "    img = np.array(img)\n",
    "    _, img = cv2.threshold(img, 140, 255, cv2.THRESH_BINARY)  # 이진화 처리\n",
    "\n",
    "    return Image.fromarray(img)\n",
    "\n",
    "def extract_text_from_image(image_path):\n",
    "    # 이미지 열기 및 전처리\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    img = preprocess_image_for_ocr(img)  \n",
    "    # OCR로 텍스트 추출 (한국어+영어)\n",
    "    text = pytesseract.image_to_string(img, lang='kor', config='--psm 6')\n",
    "    # 특수 문자 제거 (필요에 따라 조정)\n",
    "    text = re.sub(r'[^가-힣a-zA-Z0-9\\s]', '', text)  \n",
    "    return text.strip()\n",
    "\n",
    "def text_to_vector(texts):\n",
    "    vectorizer = TfidfVectorizer(max_features=500)\n",
    "    text_vectors = vectorizer.fit_transform(texts)\n",
    "    \n",
    "    # 특징 개수에 맞춰 `n_components` 동적으로 설정\n",
    "    n_components = min(50, text_vectors.shape[1])  # 최대 50차원으로 축소\n",
    "    svd = TruncatedSVD(n_components=n_components)\n",
    "    \n",
    "    text_vectors = svd.fit_transform(text_vectors)\n",
    "    return text_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted OCR Text:\n",
      "진 료 확 인 서\n",
      "차 트 번 호\n",
      "연 번 호  르\n",
      "며 늬 네     술\n",
      "신웜뿌리뭄뭄솜 동 반 한 요 주 및 기타 주 간 판 장 애 65519 951\n",
      " 변 훨 일 까 지 2 \n",
      "2023 년 06 월 15 일 부 터\n",
      "2023 년 06 뭘 15 일 까 지 1  일 간 \n",
      "실 총 원 열 자  15 일 콜 \n",
      " 홍7l외 같 이 진 료 받 았 음 을 확 연 힙 니 다  2\n",
      "발 행 일 2023 년 06 월 27 일\n",
      "의 사 성 영\n",
      "면 허 번 호\n",
      "   물 000000  7 으 \n",
      "전 화 변 호  0553132500 40 0553132501\n",
      "드0 한 인다\n",
      "의 0 기 29 장 들 률 룰 1 빠 미 \n",
      "후윅웅숍 중 악 \n",
      "틱 교\n",
      "\n",
      "Text Vector (First 10 values):\n",
      "[1.]\n"
     ]
    }
   ],
   "source": [
    "# Tesseract 실행 파일 경로 설정 (예시)\n",
    "pytesseract.pytesseract.tesseract_cmd = '/usr/bin/tesseract'\n",
    "\n",
    "# 특정 이미지 경로 지정 (예: OCR 처리를 확인할 이미지 파일 이름)\n",
    "image_path = \"/root/data/train/0d6a14437ad1a20e.jpg\"\n",
    "\n",
    "# OCR 처리된 텍스트 추출 및 출력\n",
    "extracted_text = extract_text_from_image(image_path)\n",
    "print(\"Extracted OCR Text:\")\n",
    "print(extracted_text)\n",
    "\n",
    "# 텍스트 벡터화\n",
    "text_vector = text_to_vector([extracted_text])[0]  # 벡터화 후 첫 번째 결과 가져오기\n",
    "print(\"\\nText Vector (First 10 values):\")\n",
    "print(text_vector[:10])  # 첫 10개의 벡터 값만 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5, gamma=2.0, weight=None):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.alpha = alpha  # Focal Loss \n",
    "        self.gamma = gamma  # Focal Loss의 \n",
    "        self.weight = weight  # Cross-Entropy \n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        # Cross-Entropy \n",
    "        cross_entropy_loss = F.cross_entropy(outputs, targets, weight=self.weight)\n",
    "\n",
    "        # Focal Loss \n",
    "        ce_loss = F.cross_entropy(outputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)  # 예측 확률\n",
    "        focal_loss = ((1 - pt) ** self.gamma * ce_loss).mean()\n",
    "\n",
    "        total_loss = self.alpha * focal_loss + (1 - self.alpha) * cross_entropy_loss\n",
    "        return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mixed Precision Training\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "scaler = GradScaler()  # Mixed Precision Training을 위한 스케일러 초기화\n",
    "\n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    for image, text, targets in pbar:\n",
    "        image, text, targets = image.to(device), text.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast():  # Mixed Precision 적용\n",
    "            preds = model(image, text)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    ret = {\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"train_f1\": train_f1,\n",
    "    }\n",
    "\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# data config\n",
    "data_path = '/root/data'\n",
    "\n",
    "# model config\n",
    "model_name = 'tf_efficientnetv2_m' # 'resnet50' 'efficientnet-b0', ...\n",
    "\n",
    "# training config\n",
    "img_size = 288\n",
    "LR = 1e-4\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "num_workers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터에 대한 Transform 코드\n",
    "trn_transform = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.RandomResizedCrop(height=img_size, width=img_size, scale=(0.8, 1.2), ratio=(0.75, 1.33), p=0.5),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.Rotate(limit=(-40,40), p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    A.RandomGamma(p=0.3),\n",
    "    A.CoarseDropout(max_holes=8, max_height=16, max_width=16, min_holes=1, fill_value=0, p=0.5),\n",
    "    A.MotionBlur(blur_limit=5, p=0.2),\n",
    "    A.GaussianBlur(blur_limit=(3,7), p=0.2),\n",
    "    A.GaussNoise(always_apply=False, var_limit=(50.0, 200.0), p=0.5, per_channel=True, mean= 0.0),\n",
    "    A.Affine(shear=15, rotate=10, scale=(0.9, 1.1), p=0.5),\n",
    "    A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=0.3),\n",
    "    A.GridDistortion(p=0.3),\n",
    "    A.ImageCompression(quality_lower=70, quality_upper=100, p=0.5),\n",
    "    A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.3), \n",
    "    A.GridDropout(ratio=0.5, holes_number_x=3, holes_number_y=3, p=0.3),  # GridMask\n",
    "    A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), p=0.3),\n",
    "    A.ChannelShuffle(p=0.1),\n",
    "    A.RandomShadow(shadow_roi=(0, 0.5, 1, 1), num_shadows_lower=1, num_shadows_upper=2, shadow_dimension=5, p=0.2),\n",
    "    A.Sharpen(alpha=(0.2, 0.5), lightness=(0.5, 1.0), p=0.2),  # 이미지 선명화 추가\n",
    "    A.Normalize(mean=[0.5805, 0.5895, 0.5944], std=[0.186, 0.183, 0.187]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# 클래스 3과 7에 대한 transform (cutout 비율 0.4)\n",
    "transform_p_0_4 = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.RandomResizedCrop(height=img_size, width=img_size, scale=(0.8, 1.2), ratio=(0.75, 1.33), p=0.5),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.Rotate(limit=(-40, 40), p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    A.RandomGamma(p=0.3),\n",
    "    # CoarseDropout의 p 값을 0.4로 설정\n",
    "    A.CoarseDropout(max_holes=8, max_height=16, max_width=16, min_holes=1, fill_value=0, p=0.4),\n",
    "    # 나머지 transform은 동일\n",
    "    A.MotionBlur(blur_limit=5, p=0.2),\n",
    "    A.GaussianBlur(blur_limit=(3, 7), p=0.2),\n",
    "    A.GaussNoise(var_limit=(50.0, 200.0), p=0.5, per_channel=True, mean=0.0),\n",
    "    A.Affine(shear=15, rotate=10, scale=(0.9, 1.1), p=0.5),\n",
    "    A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=0.3),\n",
    "    A.GridDistortion(p=0.3),\n",
    "    A.ImageCompression(quality_lower=70, quality_upper=100, p=0.5),\n",
    "    A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.3),\n",
    "    A.GridDropout(ratio=0.5, holes_number_x=3, holes_number_y=3, p=0.3),\n",
    "    A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), p=0.3),\n",
    "    A.ChannelShuffle(p=0.1),\n",
    "    A.RandomShadow(shadow_roi=(0, 0.5, 1, 1), num_shadows_lower=1, num_shadows_upper=2, shadow_dimension=5, p=0.2),\n",
    "    A.Sharpen(alpha=(0.2, 0.5), lightness=(0.5, 1.0), p=0.2),\n",
    "    A.Normalize(mean=[0.5805, 0.5895, 0.5944], std=[0.186, 0.183, 0.187]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# 클래스 4와 14에 대한 transform (cutout 비율 0.45)\n",
    "transform_p_0_45 = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.RandomResizedCrop(height=img_size, width=img_size, scale=(0.8, 1.2), ratio=(0.75, 1.33), p=0.5),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.Rotate(limit=(-40, 40), p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    A.RandomGamma(p=0.3),\n",
    "    # CoarseDropout의 p 값을 0.45로 설정\n",
    "    A.CoarseDropout(max_holes=8, max_height=16, max_width=16, min_holes=1, fill_value=0, p=0.45),\n",
    "    # 나머지 transform은 동일\n",
    "    A.MotionBlur(blur_limit=5, p=0.2),\n",
    "    A.GaussianBlur(blur_limit=(3, 7), p=0.2),\n",
    "    A.GaussNoise(var_limit=(50.0, 200.0), p=0.5, per_channel=True, mean=0.0),\n",
    "    A.Affine(shear=15, rotate=10, scale=(0.9, 1.1), p=0.5),\n",
    "    A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=0.3),\n",
    "    A.GridDistortion(p=0.3),\n",
    "    A.ImageCompression(quality_lower=70, quality_upper=100, p=0.5),\n",
    "    A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.3),\n",
    "    A.GridDropout(ratio=0.5, holes_number_x=3, holes_number_y=3, p=0.3),\n",
    "    A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), p=0.3),\n",
    "    A.ChannelShuffle(p=0.1),\n",
    "    A.RandomShadow(shadow_roi=(0, 0.5, 1, 1), num_shadows_lower=1, num_shadows_upper=2, shadow_dimension=5, p=0.2),\n",
    "    A.Sharpen(alpha=(0.2, 0.5), lightness=(0.5, 1.0), p=0.2),\n",
    "    A.Normalize(mean=[0.5805, 0.5895, 0.5944], std=[0.186, 0.183, 0.187]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# 클래스별 transform을 딕셔너리로 정의\n",
    "class_transforms = {\n",
    "    3: transform_p_0_4,\n",
    "    7: transform_p_0_4,\n",
    "    4: transform_p_0_45,\n",
    "    14: transform_p_0_45,\n",
    "}\n",
    "\n",
    "# 테스트 데이터에 대한 Transform 코드\n",
    "tst_transform = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.Normalize(mean=[0.5805, 0.5895, 0.5944], std=[0.186, 0.183, 0.187]),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6936 3140\n"
     ]
    }
   ],
   "source": [
    "# train.csv 파일 불러오기\n",
    "train_df = pd.read_csv(\"/root/data/train.csv\")\n",
    "\n",
    "# 특정 행의 target 값 수정\n",
    "train_df.loc[428, 'target'] = 7\n",
    "train_df.loc[1095, 'target'] = 14\n",
    "train_df.loc[862, 'target'] = 3\n",
    "train_df.loc[192, 'target'] = 7\n",
    "train_df.loc[1237, 'target'] = 14\n",
    "train_df.loc[38, 'target'] = 10\n",
    "train_df.loc[340, 'target'] = 10\n",
    "\n",
    "# 변경된 DataFrame을 다시 train.csv에 저장\n",
    "train_df.to_csv(\"/root/data/train.csv\", index=False)\n",
    "\n",
    "# Dataset 정의\n",
    "trn_dataset = ImageDataset(\n",
    "    \"/root/data/train.csv\",\n",
    "    \"/root/data/train/\",\n",
    "    transform=trn_transform,\n",
    "    oversample=True\n",
    ")\n",
    "tst_dataset = ImageDataset(\n",
    "    \"/root/data/sample_submission.csv\",\n",
    "    \"/root/data/test/\",\n",
    "    transform=tst_transform\n",
    ")\n",
    "\n",
    "# 데이터셋 크기 확인\n",
    "print(len(trn_dataset), len(tst_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader 정의\n",
    "trn_loader = DataLoader(\n",
    "    trn_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=False,\n",
    "    drop_last=False\n",
    ")\n",
    "tst_loader = DataLoader(\n",
    "    tst_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalModel(nn.Module):\n",
    "    def __init__(self, model_name, text_dim=50, num_classes=17, text_weight=1.5):\n",
    "        super(MultimodalModel, self).__init__()\n",
    "        self.image_model = timm.create_model(model_name, pretrained=True, num_classes=0)  # 최종 분류 레이어 제거\n",
    "        self.text_fc = nn.Linear(text_dim, 256)  # 텍스트 벡터를 위한 추가 레이어\n",
    "        self.classifier = nn.Linear(256 + self.image_model.num_features, num_classes)  # 이미지와 텍스트 특징 결합\n",
    "        self.text_weight = text_weight\n",
    "\n",
    "    def forward(self, image, text):\n",
    "        img_features = self.image_model(image)\n",
    "        text_features = self.text_fc(text)\n",
    "        \n",
    "        # 이미지와 텍스트 특징을 결합하여 최종 분류 레이어에 전달\n",
    "        combined_features = torch.cat([img_features, text_features], dim=1)\n",
    "        return self.classifier(combined_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91803834df9d455fa3ca2944a972dab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/218M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_weights = torch.tensor([1.0] * 17).to(device) \n",
    "class_weights[[3, 4, 7, 14]] *= 2 \n",
    "\n",
    "model = MultimodalModel(\n",
    "    model_name=model_name,\n",
    "    text_dim=50,  # 텍스트 벡터의 차원\n",
    "    num_classes=17\n",
    ").to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1,weight=class_weights)\n",
    "#loss_fn = CombinedLoss(alpha=0.5, gamma=2.0, weight=class_weights).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=LR, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Train Loss: 1.5258\n",
      "Train F1 Score: 0.6537\n",
      "\n",
      "Epoch 2\n",
      "Train Loss: 0.9168\n",
      "Train F1 Score: 0.8940\n",
      "\n",
      "Epoch 3\n",
      "Train Loss: 0.8237\n",
      "Train F1 Score: 0.9258\n",
      "\n",
      "Epoch 4\n",
      "Train Loss: 0.7697\n",
      "Train F1 Score: 0.9431\n",
      "\n",
      "Epoch 5\n",
      "Train Loss: 0.7277\n",
      "Train F1 Score: 0.9565\n",
      "\n",
      "Epoch 6\n",
      "Train Loss: 0.7041\n",
      "Train F1 Score: 0.9644\n",
      "\n",
      "Epoch 7\n",
      "Train Loss: 0.6859\n",
      "Train F1 Score: 0.9705\n",
      "\n",
      "Epoch 8\n",
      "Train Loss: 0.6694\n",
      "Train F1 Score: 0.9735\n",
      "\n",
      "Epoch 9\n",
      "Train Loss: 0.6536\n",
      "Train F1 Score: 0.9792\n",
      "\n",
      "Epoch 10\n",
      "Train Loss: 0.6573\n",
      "Train F1 Score: 0.9771\n",
      "\n",
      "Epoch 11\n",
      "Train Loss: 0.6475\n",
      "Train F1 Score: 0.9798\n",
      "\n",
      "Epoch 12\n",
      "Train Loss: 0.6302\n",
      "Train F1 Score: 0.9859\n",
      "\n",
      "Epoch 13\n",
      "Train Loss: 0.6367\n",
      "Train F1 Score: 0.9845\n",
      "\n",
      "Epoch 14\n",
      "Train Loss: 0.6277\n",
      "Train F1 Score: 0.9850\n",
      "\n",
      "Epoch 15\n",
      "Train Loss: 0.6183\n",
      "Train F1 Score: 0.9886\n",
      "\n",
      "Epoch 16\n",
      "Train Loss: 0.6311\n",
      "Train F1 Score: 0.9847\n",
      "\n",
      "Epoch 17\n",
      "Train Loss: 0.6277\n",
      "Train F1 Score: 0.9866\n",
      "\n",
      "Epoch 00018: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch 18\n",
      "Train Loss: 0.6185\n",
      "Train F1 Score: 0.9876\n",
      "\n",
      "Epoch 19\n",
      "Train Loss: 0.6014\n",
      "Train F1 Score: 0.9919\n",
      "\n",
      "Epoch 20\n",
      "Train Loss: 0.5982\n",
      "Train F1 Score: 0.9925\n",
      "\n",
      "Epoch 21\n",
      "Train Loss: 0.5945\n",
      "Train F1 Score: 0.9952\n",
      "\n",
      "Epoch 22\n",
      "Train Loss: 0.5923\n",
      "Train F1 Score: 0.9948\n",
      "\n",
      "Epoch 23\n",
      "Train Loss: 0.5973\n",
      "Train F1 Score: 0.9922\n",
      "\n",
      "Epoch 00024: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch 24\n",
      "Train Loss: 0.5944\n",
      "Train F1 Score: 0.9941\n",
      "\n",
      "Epoch 25\n",
      "Train Loss: 0.5905\n",
      "Train F1 Score: 0.9942\n",
      "\n",
      "Epoch 26\n",
      "Train Loss: 0.5874\n",
      "Train F1 Score: 0.9963\n",
      "\n",
      "Epoch 27\n",
      "Train Loss: 0.5869\n",
      "Train F1 Score: 0.9960\n",
      "\n",
      "Epoch 28\n",
      "Train Loss: 0.5823\n",
      "Train F1 Score: 0.9971\n",
      "\n",
      "Epoch 29\n",
      "Train Loss: 0.5801\n",
      "Train F1 Score: 0.9975\n",
      "\n",
      "Epoch 30\n",
      "Train Loss: 0.5811\n",
      "Train F1 Score: 0.9968\n",
      "\n",
      "Epoch 31\n",
      "Train Loss: 0.5816\n",
      "Train F1 Score: 0.9971\n",
      "\n",
      "Epoch 00032: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch 32\n",
      "Train Loss: 0.5872\n",
      "Train F1 Score: 0.9950\n",
      "\n",
      "Epoch 33\n",
      "Train Loss: 0.5827\n",
      "Train F1 Score: 0.9958\n",
      "\n",
      "Early stopping due to no improvement in F1 score.\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "# 조기 종료 설정\n",
    "patience = 5  # 개선되지 않는 에포크 수\n",
    "best_f1 = 0   # 최고 F1 스코어\n",
    "early_stopping_counter = 0  # 조기 종료 카운터\n",
    "\n",
    "# 학습률 스케줄러 설정\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, verbose=True)\n",
    "#scheduler = OneCycleLR(optimizer, max_lr=LR, steps_per_epoch=len(trn_loader), epochs=EPOCHS)\n",
    "\n",
    "# 학습 루프\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    text_weight = 1.5  # 특정 클래스에서 텍스트 비중을 높이는 값\n",
    "\n",
    "    for images, text_vectors, targets in trn_loader:\n",
    "        images, text_vectors, targets = images.to(device), text_vectors.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 특정 클래스의 경우 텍스트 비중을 높임\n",
    "        mask = torch.isin(targets, torch.tensor([3, 4, 7, 14], device=device))\n",
    "        text_vectors = torch.where(mask.unsqueeze(1), text_vectors * text_weight, text_vectors)\n",
    "\n",
    "        # 모델에 이미지와 텍스트 벡터를 전달\n",
    "        outputs = model(images, text_vectors)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(outputs.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "    train_loss /= len(trn_loader)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    # Scheduler step을 F1 스코어 기준으로 조정\n",
    "    scheduler.step(train_f1)\n",
    "\n",
    "    # F1 스코어 개선 확인\n",
    "    if train_f1 > best_f1:\n",
    "        best_f1 = train_f1  # 최고 F1 갱신\n",
    "        early_stopping_counter = 0  # 카운터 초기화\n",
    "    else:\n",
    "        early_stopping_counter += 1  # 개선되지 않으면 카운터 증가\n",
    "\n",
    "    # 조기 종료 조건 확인\n",
    "    if early_stopping_counter >= patience:\n",
    "        print(\"Early stopping due to no improvement in F1 score.\")\n",
    "        break  # 학습 중단\n",
    "\n",
    "    # 로그 출력\n",
    "    log = f\"Epoch {epoch + 1}\\nTrain Loss: {train_loss:.4f}\\nTrain F1 Score: {train_f1:.4f}\\n\"\n",
    "    print(log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:22<00:00,  4.31it/s]\n"
     ]
    }
   ],
   "source": [
    "preds_list = []\n",
    "\n",
    "model.eval()\n",
    "for image, _, _ in tqdm(tst_loader):   # tst_loader에서는 image와 target만 반환\n",
    "    image = image.to(device)\n",
    "\n",
    "    # 더미 텍스트 벡터 생성 (예: 크기 50의 제로 텐서)\n",
    "    text = torch.zeros((image.size(0), 50)).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds = model(image, text)  # 더미 텍스트 벡터도 함께 전달\n",
    "    preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame(tst_dataset.df, columns=['ID', 'target'])\n",
    "pred_df['target'] = preds_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission_df = pd.read_csv(\"/root/data/sample_submission.csv\")\n",
    "assert (sample_submission_df['ID'] == pred_df['ID']).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.to_csv(\"/root/data/output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0008fdb22ddce0ce.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00091bffdffd83de.jpg</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00396fbc1f6cc21d.jpg</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00471f8038d9c4b6.jpg</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00901f504008d884.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     ID  target\n",
       "0  0008fdb22ddce0ce.jpg       2\n",
       "1  00091bffdffd83de.jpg      12\n",
       "2  00396fbc1f6cc21d.jpg       5\n",
       "3  00471f8038d9c4b6.jpg      12\n",
       "4  00901f504008d884.jpg       2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
